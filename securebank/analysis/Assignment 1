import pandas as pd
import json
import matplotlib.pyplot as plt
import seaborn as sns

# Set the aesthetic style of the plots for better visuals
sns.set(style="whitegrid")

# File paths to the data sources
customer_file_path = '/Users/katevodeiko/Downloads/AS 1/customer_release.csv'
transactions_file_path = '/Users/katevodeiko/Downloads/AS 1/transactions_release.parquet'
fraud_file_path = '/Users/katevodeiko/Downloads/AS 1/fraud_release.json'

# Task 1: Function to merge data from multiple sources
def __merge(customer_filename: str, transaction_filename: str, fraud_filename: str):
    """
    Merges the customer, transaction, and fraud data into a single DataFrame.
    
    Args:
    - customer_filename: str, path to the customer CSV file.
    - transaction_filename: str, path to the transaction Parquet file.
    - fraud_filename: str, path to the fraud JSON file.
    
    Returns:
    - final_data: pd.DataFrame, merged and sorted DataFrame indexed by transaction number.
    """
    # Load the customer data from the CSV file
    customer_data = pd.read_csv(customer_filename)
    
    # Load the transaction data from the Parquet file
    transactions_data = pd.read_parquet(transaction_filename)
    
    # Load the fraud data from the JSON file
    with open(fraud_filename, 'r') as file:
        fraud_dict = json.load(file)
    
    # Convert the fraud dictionary to a DataFrame
    fraud_data = pd.DataFrame(list(fraud_dict.items()), columns=['trans_num', 'is_fraud'])

    # Merge transaction data with fraud data on 'trans_num'
    merged_data = transactions_data.merge(fraud_data, on='trans_num', how='left')

    # Merge the resulting data with customer data on 'cc_num'
    final_data = merged_data.merge(customer_data, on='cc_num', how='left')

    # Sort the final DataFrame by transaction date and time for chronological analysis
    final_data.sort_values(by='trans_date_trans_time', inplace=True)

    # Set 'trans_num' as the index for easier lookups and data integrity
    final_data.set_index('trans_num', inplace=True)

    return final_data

# Example usage of __merge function
final_data = __merge(customer_file_path, transactions_file_path, fraud_file_path)

# Task 2: Data Analysis

# 1. Exploring the Data
# Display basic information about the DataFrame including data types and non-null counts
print("Data Information:")
print(final_data.info())

# Display summary statistics for numerical columns to understand the distribution
print("\nData Description:")
print(final_data.describe())

# Check for missing values in the dataset, which could indicate data quality issues
print("\nMissing Values in Data:")
print(final_data.isnull().sum())  # Check for missing values

# 2. Visualizing the Data

# Example 1: Distribution of Fraud vs Non-Fraud Transactions
# This plot visualizes the frequency of fraudulent versus non-fraudulent transactions
plt.figure(figsize=(10, 6))
sns.countplot(data=final_data, x='is_fraud', palette='viridis')
plt.title('Distribution of Fraud vs Non-Fraud Transactions', fontsize=16)
plt.xlabel('Is Fraud', fontsize=14)
plt.ylabel('Count', fontsize=14)
plt.xticks(ticks=[0, 1], labels=['Non-Fraudulent', 'Fraudulent'])
plt.show()

# Example 2: Transaction Amount by Fraud Status
# This box plot compares the distribution of transaction amounts between fraudulent and non-fraudulent transactions
plt.figure(figsize=(10, 6))
sns.boxplot(data=final_data, x='is_fraud', y='amt', palette='coolwarm')
plt.title('Transaction Amounts by Fraud Status', fontsize=16)
plt.xlabel('Is Fraud', fontsize=14)
plt.ylabel('Transaction Amount', fontsize=14)
plt.xticks(ticks=[0, 1], labels=['Non-Fraudulent', 'Fraudulent'])
plt.yscale('log')  # Use log scale to better visualize transactions with large variances
plt.show()

# Example 3: Transactions Over Time
# Convert the 'trans_date_trans_time' column to datetime format for time series analysis
final_data['trans_date_trans_time'] = pd.to_datetime(final_data['trans_date_trans_time'])
final_data.set_index('trans_date_trans_time', inplace=True)

# Plot the total transaction amount over time to observe trends or anomalies in transaction behavior
plt.figure(figsize=(14, 7))
final_data.groupby(final_data.index.date)['amt'].sum().plot(color='dodgerblue', linewidth=2)
plt.title('Total Transaction Amount Over Time', fontsize=16)
plt.xlabel('Date', fontsize=14)
plt.ylabel('Total Transaction Amount', fontsize=14)
plt.grid(True)
plt.show()

# 3. Corroborating or Disproving Analysts' Insights

# Insight 1: High-value transactions are more likely to be fraudulent
# Analyzing whether higher transaction amounts correlate with a higher incidence of fraud
high_value_fraud = final_data[final_data['amt'] > 1000]['is_fraud'].value_counts(normalize=True)
print("High-value transactions - Fraud distribution:", high_value_fraud)

# Insight 2: Fraudulent transactions occur more frequently during certain times of the day
# Analyzing the hourly distribution of fraudulent transactions
final_data['hour'] = final_data.index.hour
plt.figure(figsize=(14, 7))
fraud_by_hour = final_data.groupby('hour')['is_fraud'].mean()
fraud_by_hour.plot(kind='bar', color='salmon', edgecolor='black')
plt.title('Average Fraud Rate by Hour of Day', fontsize=16)
plt.xlabel('Hour', fontsize=14)
plt.ylabel('Average Fraud Rate', fontsize=14)
plt.grid(True)
plt.show()

# 4. Two Additional Insights

# Insight 1: Certain job roles may have a higher likelihood of fraud
# Identify job roles that show a higher incidence of fraudulent transactions
job_fraud = final_data.groupby('job')['is_fraud'].mean().sort_values(ascending=False).head(10)
print("\nTop 10 job roles by fraud likelihood:\n", job_fraud)

# Insight 2: Geographic analysis - are certain states more prone to fraud?
# Analyze fraud rates across different states to identify geographic patterns in fraudulent activity
state_fraud = final_data.groupby('state')['is_fraud'].mean().sort_values(ascending=False).head(10)
print("\nTop 10 states by fraud likelihood:\n", state_fraud)
